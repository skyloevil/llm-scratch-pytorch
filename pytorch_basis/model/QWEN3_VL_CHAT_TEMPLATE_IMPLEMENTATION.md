# Qwen3-VL Chat Template åœ¨ vLLM ä¸­çš„å®ç°ä½ç½®

## æ¦‚è¿°

Qwen3-VL çš„ chat template åœ¨ vLLM ä¸­**ä¸æ˜¯å•ç‹¬å®ç°çš„**ï¼Œè€Œæ˜¯é€šè¿‡ä»¥ä¸‹æ–¹å¼å¤„ç†ï¼š

1. **ä¼˜å…ˆä½¿ç”¨ Hugging Face Tokenizer è‡ªå¸¦çš„ chat template**
2. **Fallback åˆ° vLLM æä¾›çš„é»˜è®¤ ChatML æ¨¡æ¿**

## æ ¸å¿ƒå®ç°æµç¨‹

### 1. Chat Template è§£æå…¥å£

**æ–‡ä»¶**: [vllm/entrypoints/chat_utils.py](vllm/entrypoints/chat_utils.py)

**å…³é”®å‡½æ•°**: `resolve_hf_chat_template()` (Line 489-534)

```python
def resolve_hf_chat_template(
    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,
    chat_template: str | None,
    tools: list[dict[str, Any]] | None,
    *,
    model_config: ModelConfig,
) -> str | None:
    # 1st priority: ç”¨æˆ·æŒ‡å®šçš„ chat template
    if chat_template is not None:
        return chat_template

    # 2nd priority: AutoProcessor chat template (é™¤éå¯ç”¨äº† tool calling)
    if tools is None:
        chat_template = _try_get_processor_chat_template(tokenizer, model_config)
        if chat_template is not None:
            return chat_template

    # 3rd priority: AutoTokenizer chat template
    try:
        return tokenizer.get_chat_template(chat_template, tools=tools)
    except Exception:
        logger.debug("Failed to load AutoTokenizer chat template...")

    # 4th priority: vLLM é¢„å®šä¹‰çš„ fallback templates
    path = get_chat_template_fallback_path(
        model_type=model_config.hf_config.model_type,
        tokenizer_name_or_path=model_config.tokenizer,
    )
    if path is not None:
        chat_template = load_chat_template(path)

    return chat_template
```

### 2. Qwen ç³»åˆ—çš„ Fallback ç­–ç•¥

**æ–‡ä»¶**: [vllm/transformers_utils/chat_templates/registry.py](vllm/transformers_utils/chat_templates/registry.py)

**å…³é”®é…ç½®** (Line 32-44):

```python
_MODEL_TYPE_TO_CHAT_TEMPLATE_FALLBACK: dict[str, ChatTemplatePath] = {
    "qwen": _get_qwen_chat_template_fallback,  # åŠ¨æ€é€‰æ‹©
    # ... å…¶ä»–æ¨¡å‹
}

def _get_qwen_chat_template_fallback(tokenizer_name_or_path: str) -> Path | None:
    # Qwen-Chat ç³»åˆ—ä½¿ç”¨ ChatML æ¨¡æ¿
    if tokenizer_name_or_path.endswith("-Chat"):
        return CHAT_TEMPLATES_DIR / "template_chatml.jinja"

    # å…¶ä»– Qwen ç³»åˆ—ä½¿ç”¨åŸºç¡€æ¨¡æ¿
    return CHAT_TEMPLATES_DIR / "template_basic.jinja"
```

**Qwen3-VL çš„ model_type**: `qwen` (ä¸ Qwen2ã€Qwen2-VL ç›¸åŒ)

### 3. ChatML æ¨¡æ¿å†…å®¹

**æ–‡ä»¶**: [vllm/transformers_utils/chat_templates/template_chatml.jinja](vllm/transformers_utils/chat_templates/template_chatml.jinja)

```jinja
{%- for message in messages %}
    {{- '<|im_start|>' + message['role'] + '\n' }}
    {%- if message['content'] is string %}
        {{- message['content'] + '<|im_end|>\n' }}
    {%- else %}
        {%- for content in message['content'] %}
            {%- if content['type'] == 'text' %}
                {{- content['text'] }}
            {%- endif %}
        {%- endfor %}
        {{- '<|im_end|>\n' }}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '<|im_start|>assistant\n' }}
{%- endif %}
```

## Qwen3-VL çš„å®é™…å¤„ç†

### æƒ…å†µ 1: Qwen3-VL ä½¿ç”¨ HF Tokenizer è‡ªå¸¦æ¨¡æ¿ âœ…

**å®é™…æƒ…å†µ**: Qwen3-VL çš„ tokenizer å·²åŒ…å«å®Œæ•´çš„ chat template

```bash
# éªŒè¯
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-VL-4B-Instruct')
print('Has chat_template:', hasattr(tokenizer, 'chat_template'))
print('Chat template length:', len(tokenizer.chat_template) if tokenizer.chat_template else 0)
"
```

**ç»“æœ**:
- âœ… Qwen3-VL tokenizer åŒ…å« chat_template
- âœ… æ”¯æŒå®Œæ•´çš„å¤šæ¨¡æ€ (image/video/audio)
- âœ… æ”¯æŒ tool calling

**ä¼˜å…ˆçº§**: vLLM ä¼šä½¿ç”¨ **HF tokenizer è‡ªå¸¦çš„ template**ï¼Œä¸ä¼šä½¿ç”¨ fallback

### æƒ…å†µ 2: å¦‚æœ Tokenizer ç¼ºå¤±æ¨¡æ¿ (å‡è®¾åœºæ™¯)

å¦‚æœ Qwen3-VL tokenizer æ²¡æœ‰ chat_templateï¼ŒvLLM ä¼šï¼š

1. æ£€æµ‹ `model_type = "qwen3_vl"` æˆ– `"qwen"`
2. è°ƒç”¨ `_get_qwen_chat_template_fallback()`
3. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦ä»¥ `-Chat` æˆ– `-Instruct` ç»“å°¾
4. è¿”å› `template_chatml.jinja`

## å¤šæ¨¡æ€å†…å®¹çš„å¤„ç†

### Placeholder æ›¿æ¢æœºåˆ¶

**æ–‡ä»¶**: [vllm/entrypoints/chat_utils.py](vllm/entrypoints/chat_utils.py)

**å…³é”®å¸¸é‡** (Line 60-64):

```python
MODALITY_PLACEHOLDERS_MAP = {
    "image": "<##IMAGE##>",
    "audio": "<##AUDIO##>",
    "video": "<##VIDEO##>",
}
```

### å¤šæ¨¡æ€å†…å®¹è§£ææµç¨‹

1. **è§£æ OpenAI æ ¼å¼çš„æ¶ˆæ¯** â†’ `parse_chat_messages()` (Line 1592)
   - æå– image/video/audio URL
   - ä¸‹è½½/åŠ è½½å¤šæ¨¡æ€æ•°æ®
   - æ’å…¥æ¨¡å‹ç‰¹å®šçš„ placeholder

2. **è·å–æ¨¡å‹ Placeholder** â†’ `model.get_placeholder_str()` (Line 687)
   - Qwen3-VL å®ç°åœ¨: [vllm/model_executor/models/qwen3_vl.py:1243-1248](vllm/model_executor/models/qwen3_vl.py#L1243-L1248)

   ```python
   @classmethod
   def get_placeholder_str(cls, modality: str, i: int) -> str | None:
       if modality.startswith("image"):
           return "<|vision_start|><|image_pad|><|vision_end|>"
       if modality.startswith("video"):
           return "<|vision_start|><|video_pad|><|vision_end|>"
       raise ValueError("Only image or video modality is supported")
   ```

3. **åº”ç”¨ Chat Template** â†’ `tokenizer.apply_chat_template()` (Line 1764)
   - ä½¿ç”¨ HF tokenizer çš„ Jinja2 æ¨¡æ¿
   - æ›¿æ¢å ä½ç¬¦
   - ç”Ÿæˆæœ€ç»ˆ prompt

## å®Œæ•´è°ƒç”¨é“¾

```
OpenAI API Request (messages)
    â†“
[chat_utils.py] parse_chat_messages()
    â†“
[chat_utils.py] _parse_chat_message_content_part()
    â†“
[chat_utils.py] MultiModalContentParser.parse_video()
    â†“
[qwen3_vl.py] Qwen3VLForConditionalGeneration.get_placeholder_str()
    â†’ è¿”å› "<|vision_start|><|video_pad|><|vision_end|>"
    â†“
[chat_utils.py] apply_hf_chat_template()
    â†“
[chat_utils.py] resolve_hf_chat_template()
    â†’ 1. å°è¯• user-provided template
    â†’ 2. å°è¯• AutoProcessor.chat_template
    â†’ 3. å°è¯• AutoTokenizer.chat_template âœ… (Qwen3-VL åœ¨è¿™é‡ŒæˆåŠŸ)
    â†’ 4. Fallback to vLLM template_chatml.jinja
    â†“
[transformers] tokenizer.apply_chat_template()
    â†’ ä½¿ç”¨ Qwen3-VL çš„ Jinja2 template
    â†’ å°† "<|vision_start|><|video_pad|><|vision_end|>" ä¿æŒåŸæ ·
    â†“
Final Prompt String
```

## å®é™…ç¤ºä¾‹

### è¾“å…¥ (OpenAI æ ¼å¼)

```json
{
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "video", "video_url": {"url": "file:///path/to/video.mp4"}},
        {"type": "text", "text": "Describe this video."}
      ]
    }
  ]
}
```

### å¤„ç†æµç¨‹

1. **parse_chat_messages()** æå–:
   - video_url: `file:///path/to/video.mp4`
   - text: `Describe this video.`

2. **MultiModalContentParser** å¤„ç†:
   - ä¸‹è½½/åŠ è½½è§†é¢‘æ•°æ®
   - è°ƒç”¨ `get_placeholder_str("video", 1)`
   - è¿”å›: `"<|vision_start|><|video_pad|><|vision_end|>"`

3. **æ„å»º conversation**:
   ```python
   [
       {
           "role": "user",
           "content": [
               {"type": "video"},
               {"type": "text", "text": "Describe this video."}
           ]
       }
   ]
   ```

4. **apply_chat_template()** ç”Ÿæˆ:
   ```
   <|im_start|>user
   <|vision_start|><|video_pad|><|vision_end|>Describe this video.<|im_end|>
   <|im_start|>assistant
   ```

## å…³é”®æ–‡ä»¶ä½ç½®æ€»ç»“

| æ–‡ä»¶ | ä½œç”¨ | å…³é”®å‡½æ•°/ç±» |
|------|------|-------------|
| `vllm/entrypoints/chat_utils.py` | Chat template è§£æä¸»é€»è¾‘ | `resolve_hf_chat_template()`<br>`parse_chat_messages()`<br>`apply_hf_chat_template()` |
| `vllm/transformers_utils/chat_templates/registry.py` | Fallback template æ³¨å†Œè¡¨ | `get_chat_template_fallback_path()`<br>`_get_qwen_chat_template_fallback()` |
| `vllm/transformers_utils/chat_templates/template_chatml.jinja` | ChatML æ¨¡æ¿æ–‡ä»¶ | Jinja2 template |
| `vllm/model_executor/models/qwen3_vl.py` | Qwen3-VL æ¨¡å‹å®ç° | `get_placeholder_str()`<br>`Qwen3VLMultiModalProcessor` |

## éªŒè¯æ–¹æ³•

### 1. æŸ¥çœ‹å®é™…ä½¿ç”¨çš„ Chat Template

```python
from vllm import LLM

llm = LLM(model="Qwen/Qwen3-VL-4B-Instruct")
tokenizer = llm.llm_engine.tokenizer

# æŸ¥çœ‹ chat template æ¥æº
print("Chat template source:", "HF Tokenizer" if tokenizer.chat_template else "vLLM Fallback")
print("\nFirst 200 chars of template:")
print(tokenizer.chat_template[:200] if tokenizer.chat_template else "No template")
```

### 2. æµ‹è¯• Placeholder ç”Ÿæˆ

```python
from vllm.model_executor.models.qwen3_vl import Qwen3VLForConditionalGeneration

# æµ‹è¯• image placeholder
image_ph = Qwen3VLForConditionalGeneration.get_placeholder_str("image", 1)
print(f"Image placeholder: {image_ph}")

# æµ‹è¯• video placeholder
video_ph = Qwen3VLForConditionalGeneration.get_placeholder_str("video", 1)
print(f"Video placeholder: {video_ph}")
```

### 3. å®Œæ•´æµ‹è¯•

```python
from vllm import LLM

llm = LLM(model="Qwen/Qwen3-VL-4B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {"type": "video_url", "video_url": {"url": "file:///path/to/video.mp4"}},
            {"type": "text", "text": "What's in this video?"}
        ]
    }
]

# ç”Ÿæˆ (ä¼šè‡ªåŠ¨åº”ç”¨ chat template)
outputs = llm.chat(messages=messages, max_tokens=100)
print(outputs[0].outputs[0].text)
```

## å¸¸è§é—®é¢˜

### Q1: Qwen3-VL æ˜¯å¦éœ€è¦è‡ªå®šä¹‰ chat templateï¼Ÿ
A: **ä¸éœ€è¦**ã€‚Qwen3-VL çš„ tokenizer å·²ç»åŒ…å«äº†å®Œæ•´çš„ chat templateï¼ŒvLLM ä¼šè‡ªåŠ¨ä½¿ç”¨ã€‚

### Q2: å¦‚æœæƒ³è‡ªå®šä¹‰ chat template æ€ä¹ˆåŠï¼Ÿ
A: å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼:

```python
# æ–¹æ³• 1: å¯åŠ¨æ—¶æŒ‡å®š
llm = LLM(
    model="Qwen/Qwen3-VL-4B-Instruct",
    chat_template="/path/to/custom_template.jinja"
)

# æ–¹æ³• 2: API æœåŠ¡å™¨å¯åŠ¨å‚æ•°
vllm serve Qwen/Qwen3-VL-4B-Instruct \
    --chat-template /path/to/custom_template.jinja
```

### Q3: ChatML æ¨¡æ¿å’Œ Qwen3-VL åŸç”Ÿæ¨¡æ¿æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
A:
- **ChatML fallback**: ç®€åŒ–ç‰ˆæœ¬ï¼Œä»…æ”¯æŒåŸºæœ¬çš„æ–‡æœ¬å¯¹è¯
- **Qwen3-VL åŸç”Ÿæ¨¡æ¿**: å®Œæ•´ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šæ¨¡æ€ã€å·¥å…·è°ƒç”¨ã€æ€è€ƒé“¾ç­‰é«˜çº§åŠŸèƒ½

### Q4: è§†é¢‘çš„ `<|video_pad|>` ä¼šè¢«æ›¿æ¢æˆä»€ä¹ˆï¼Ÿ
A:
- åœ¨ chat template é˜¶æ®µï¼Œ`<|video_pad|>` ä¿æŒä¸å˜
- åœ¨æ¨¡å‹ forward é˜¶æ®µï¼Œä¼šè¢«æ›¿æ¢ä¸ºå®é™…çš„ video token embeddings
- æ›¿æ¢åçš„ token æ•°é‡å–å†³äºè§†é¢‘åˆ†è¾¨ç‡ã€å¸§æ•°å’Œ EVS pruning rate

## æ€»ç»“

**Qwen3-VL åœ¨ vLLM ä¸­çš„ chat template å¤„ç†æœºåˆ¶**:

1. âœ… **ä¸»è¦ä½¿ç”¨**: Hugging Face Tokenizer è‡ªå¸¦çš„ chat template
2. ğŸ”„ **Fallback**: å¦‚æœ tokenizer ç¼ºå¤±ï¼Œä½¿ç”¨ vLLM çš„ `template_chatml.jinja`
3. ğŸ¯ **Placeholder**: é€šè¿‡ `get_placeholder_str()` ç”Ÿæˆç‰¹å®šçš„è§†è§‰ token
4. ğŸ”§ **è‡ªåŠ¨å¤„ç†**: ç”¨æˆ·æ— éœ€æ‰‹åŠ¨é…ç½®ï¼ŒvLLM è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„æ¨¡æ¿

**æ ¸å¿ƒä¼˜åŠ¿**:
- æ— éœ€é¢å¤–é…ç½®
- è‡ªåŠ¨å…¼å®¹å¤šæ¨¡æ€è¾“å…¥
- ä¸ Hugging Face ç”Ÿæ€å®Œå…¨ä¸€è‡´
